{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df7BAUnL3htM"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "from albumentations import CenterCrop\n",
        "from google.colab.patches import cv2_imshow\n",
        "from patchify import patchify\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(train_path, test_path):\n",
        "    train_x = sorted(glob(os.path.join(train_path, \"training\", \"images\", \"*.tif\")))\n",
        "    train_y = sorted(glob(os.path.join(train_path, \"training\", \"1st_manual\", \"*.gif\")))\n",
        "\n",
        "    test_x = sorted(glob(os.path.join(test_path, \"test\", \"images\", \"*.tif\")))\n",
        "    test_y = sorted(glob(os.path.join(test_path, \"test\", \"1st_manual\", \"*.gif\")))\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)\n",
        "\n",
        "def augment_data(images, masks, save_path, augment=True, format=\"same\"):\n",
        "    size = (512, 512)\n",
        "\n",
        "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
        "        \n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "   \n",
        "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        if format == \"different\":\n",
        "          y = imageio.mimread(y)[0]\n",
        "        else:\n",
        "          y = cv2.imread(y, cv2.IMREAD_COLOR)\n",
        "\n",
        "\n",
        "        x = cv2.resize(x, size)\n",
        "        y = cv2.resize(y, size)\n",
        "\n",
        "        if augment == True:\n",
        "            aug = CenterCrop(height=320,width=448, always_apply=True, p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "            #print(\"\\n\",x1.shape,y1.shape)\n",
        "\n",
        "            patches_img = patchify(x1,(64,64,3),step=32)\n",
        "            #print(patches_img.shape)#(17,25,1,64,64,3): total patch created 8x8x1=64,of size:(64x64x3)\n",
        "            #print(patches_img[0].shape)\n",
        "            for i in range(patches_img.shape[0]):\n",
        "              for j in range(patches_img.shape[1]):\n",
        "                for k in range(patches_img.shape[2]):\n",
        "                  all_patch_img = patches_img[i,j,k,:,:,:]\n",
        "                  #cv2_imshow(all_patch_img)\n",
        "                  tmp_image_name = f\"{name}_{idx}{i}{j}.png\"\n",
        "                  image_path = os.path.join(save_path, \"image\",tmp_image_name)\n",
        "                  cv2.imwrite(image_path, all_patch_img)\n",
        "\n",
        "            patches_mask = patchify(y1,(64,64),step=32)\n",
        "            #print(patches_mask.shape)#(8,8,64,64): total patch created 8x8=64,of size:(64x64)\n",
        "            #print(patches_mask[0].shape)\n",
        "            for i in range(patches_mask.shape[0]):\n",
        "              for j in range(patches_mask.shape[1]):\n",
        "                \n",
        "                  all_patch_mask = patches_mask[i,j,:,:]\n",
        "                  #cv2_imshow(all_patch_img)\n",
        "                  tmp_mask_name = f\"{name}_{idx}{i}{j}.png\"\n",
        "                  image_path = os.path.join(save_path, \"mask\",tmp_mask_name)\n",
        "                  cv2.imwrite(image_path, all_patch_mask)      \n",
        "        idx+=1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    train_data_path = \"/content/drive/MyDrive/Datasets/DRIVE\"\n",
        "    test_data_path = \"/content/drive/MyDrive/Datasets/DRIVE\"\n",
        "    (train_x, train_y), (test_x, test_y) = load_data(train_data_path, test_data_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "    \"\"\" Create directories to save the augmented data \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Augmented_data/train/image\")\n",
        "    create_dir(\"/content/drive/MyDrive/Augmented_data/train/mask/\")\n",
        "    create_dir(\"/content/drive/MyDrive/Augmented_data/test/image/\")\n",
        "    create_dir(\"/content/drive/MyDrive/Augmented_data/test/mask/\")\n",
        "\n",
        "    \"\"\" Data augmentation \"\"\"\n",
        "    augment_data(train_x, train_y, \"/content/drive/MyDrive/Augmented_data/train/\", augment=True, format=\"different\")\n",
        "    augment_data(test_x, test_y, \"/content/drive/MyDrive/Augmented_data/test/\", augment=True, format=\"different\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzvNkzvS3naW"
      },
      "outputs": [],
      "source": [
        "#Making the data ready for training and Testing\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DriveDataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path):\n",
        "\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.n_samples = len(images_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
        "        image = image/255.0\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        image = image.astype(np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        \"\"\" Reading mask \"\"\"\n",
        "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = mask/255.0\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CY6sCgt3rcA"
      },
      "outputs": [],
      "source": [
        "#Loss Function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "\n",
        "        return Dice_BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfUhvje33vH3"
      },
      "outputs": [],
      "source": [
        "#Utils\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "\"\"\" Seeding the randomness. \"\"\"\n",
        "def seeding(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\"\"\" Create a directory. \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\"\"\" Calculate the time taken \"\"\"\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwdveU-f3yV2",
        "outputId": "bc30a337-fa1f-4671-8f0f-6606c6d74b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output torch.Size([1, 1, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "#RIMNet Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self,in_c,out_c, dropRate=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv2d(in_c, out_c,kernel_size=3,padding=1,bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.droprate = dropRate\n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
        "        if self.droprate > 0:\n",
        "            x = F.dropout(x, p=self.droprate, training=self.training)\n",
        "        return(x)\n",
        "\n",
        "class res_block(nn.Module): #Residual Block\n",
        "    def __init__(self, in_c, out_c, dropRate=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1=conv_block(in_c,out_c,dropRate=0.0)\n",
        "        self.conv2 =nn.Conv2d(out_c, out_c,kernel_size=3,padding=1,bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.id = nn.Conv2d(in_c, out_c, kernel_size=1,padding=0)\n",
        "        self.bn2= nn.BatchNorm2d(out_c)\n",
        "        self.droprate = dropRate    \n",
        "    def forward(self, inputs):\n",
        "        x = self.bn1(self.conv2(self.conv1(inputs)))\n",
        "        id = self.bn2(self.id(inputs))\n",
        "        x = x + id\n",
        "        x=self.relu(x)\n",
        "        if self.droprate > 0:\n",
        "            x = F.dropout(x, p=self.droprate, training=self.training)\n",
        "        return x\n",
        "\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c,dropRate=0.0):\n",
        "        super().__init__()\n",
        "        self.conv = res_block(in_c, out_c,dropRate=dropRate)\n",
        "        self.up = nn.ConvTranspose2d(out_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        up = self.up(x)\n",
        "       # print(\"Encoder output\",x.shape,p.shape)\n",
        "        return x, up\n",
        "\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c,dropRate=0.0):\n",
        "        super().__init__()      \n",
        "        self.conv1 = conv_block(in_c, out_c,dropRate=dropRate)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "        self.conv2 = res_block(out_c+out_c, out_c,dropRate=dropRate)\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.pool(x)\n",
        "        x = torch.cat([x, skip], axis=1)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class build_unet(nn.Module):\n",
        "    def __init__(self,dropRate=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        self.e1 = encoder_block(3, 16,dropRate=dropRate)\n",
        "        self.e2 = encoder_block(16, 32,dropRate=dropRate)\n",
        "        self.e3 = encoder_block(32, 64, dropRate=dropRate)\n",
        "\n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        self.b = conv_block(64, 128, dropRate=dropRate, )\n",
        "\n",
        "        \"\"\" Decoder \"\"\"\n",
        "        self.d1 = decoder_block(128, 64,dropRate=dropRate )\n",
        "        self.d2 = decoder_block(64, 32, dropRate=dropRate)\n",
        "        self.d3 = decoder_block(32, 16, dropRate=dropRate)\n",
        "\n",
        "        \"\"\" Classifier \"\"\"\n",
        "        self.outputs = nn.Conv2d(16, 1, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Encoder \"\"\"\n",
        "        c1, t1 = self.e1(inputs)\n",
        "        c2, t2 = self.e2(t1)\n",
        "        c3, t3 = self.e3(t2)\n",
        "      \n",
        "        \"\"\" Bottleneck \"\"\"\n",
        "        b = self.b(t3)\n",
        "        \n",
        "        \"\"\" Decoder \"\"\"\n",
        "        d1 = self.d1(b, c3)\n",
        "        d2 = self.d2(d1, c2)\n",
        "        d3 = self.d3(d2, c1)\n",
        "      \n",
        "        outputs = self.outputs(d3)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn((1, 3, 64,64))\n",
        "    f = build_unet(dropRate=0.08)\n",
        "    y = f(x)\n",
        "    print(\"output\",y.shape)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "gOoeMOJu31RV",
        "outputId": "a0cb16db-fb7c-458c-b803-7c743ebaef27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data loader 1053\n",
            "valid data loader 117\n",
            "Number of model parameters: 6760833\n",
            "Training Done\n",
            "Validation Done\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-06fab91f83d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;34m\"\"\" Saving the model \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-06fab91f83d1>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, loader, loss_fn, device, mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mmetrics_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Training - Validation - Testing \n",
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "from operator import add, sub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score,confusion_matrix,roc_auc_score\n",
        "\n",
        "# This function lodes the traning dataset and divide it into two non-overlapping validation and traning datasets\n",
        "\n",
        "def get_train_valid_loader(data_dir,\n",
        "                           batch_size,\n",
        "                           random_seed,\n",
        "                           kfolde=10,\n",
        "                           shuffle=True,\n",
        "                           show_sample=False,\n",
        "                           num_workers=1,\n",
        "                           pin_memory=False,\n",
        "                           shuffelthevaluditation=1):\n",
        "    \"\"\"\n",
        " \n",
        "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
        "    Params\n",
        "    ------\n",
        "    - data_dir: path directory to the dataset.\n",
        "    - batch_size: how many samples per batch to load.\n",
        "      mentioned in the paper. Only applied on the train split.\n",
        "    - random_seed: fix seed for reproducibility.\n",
        "    - kfolde: the number of foldes in cross validation\n",
        "      the validation set. Should be a float in the range [0, 1].\n",
        "    - shuffle: whether to shuffle the train/validation indices.\n",
        "    - show_sample: plot  sample grid of the dataset.\n",
        "    - num_workers: number of subprocesses to use when loading the dataset.\n",
        "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
        "      True if using GPU.\n",
        "    -shuffelthevaluditation: the folding index of validation set\n",
        "    Returns\n",
        "    -------\n",
        "    - train_loader: training set iterator.\n",
        "    - valid_loader: validation set iterator.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # load the dataset\n",
        "    train_x = sorted(glob(os.path.join(data_dir,  \"image\", \"*\")))[:2106]\n",
        "    train_y = sorted(glob(os.path.join(data_dir, \"mask\", \"*\")))[:2106]\n",
        "\n",
        "    valid_x = sorted(glob(os.path.join(data_dir, \"image\", \"*\")))[2106:]\n",
        "    valid_y = sorted(glob(os.path.join(data_dir, \"mask\", \"*\")))[2106:]\n",
        "    \n",
        "    \n",
        "    train_dataset = DriveDataset(train_x, train_y)\n",
        "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "   \n",
        "    dataset=ConcatDataset([train_dataset, valid_dataset])\n",
        "    \n",
        "    valid_size=(len(dataset)/kfolde)/(len(dataset))\n",
        "    num_train = len(dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    \n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[:split* (int( shuffelthevaluditation) -1 )] + indices[split*int( shuffelthevaluditation):], indices[split * (int( shuffelthevaluditation) -1 ):split * int( shuffelthevaluditation)]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    data_str = f\"Dataset Size:\\nTrain sampler: {len(train_sampler)} - Validsampler: {len(valid_sampler)}\\n\"\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, sampler=valid_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    ) \n",
        "    \n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "#best val accuracy\n",
        "#written to CSV file\n",
        "#K fold Cross Validation\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Ground truth \"\"\"\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "    y_true = y_true.reshape(-1)\n",
        "\n",
        "    \"\"\" Prediction \"\"\"\n",
        "    y_pred = y_pred.cpu()\n",
        "    y_pred = y_pred.detach().numpy()\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "\n",
        "    score_jaccard = jaccard_score(y_true, y_pred)\n",
        "    score_f1 = f1_score(y_true, y_pred)\n",
        "    #score_recall = recall_score(y_true, y_pred)\n",
        "    #score_precision = precision_score(y_true, y_pred)\n",
        "    #score_acc = accuracy_score(y_true, y_pred)\n",
        "    cm1=confusion_matrix(y_true,y_pred)\n",
        "    total1=sum(sum(cm1))\n",
        "    score_recall= cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    score_precision= cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
        "    score_acc = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "\n",
        "    auc=roc_auc_score(y_true, y_pred)\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice_coeff=(2*intersection +1) / (np.sum(y_true) + np.sum(y_pred)+ 1)\n",
        "\n",
        "\n",
        "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc,Specificity, Sensitivity,auc,dice_coeff]\n",
        "\n",
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0,0.0,0.0,0.0,0.0]\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, dtype=torch.float32)\n",
        "        y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        score = calculate_metrics(y, y_pred)\n",
        "        metrics_score = list(map(add, metrics_score, score))\n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    metrics_score = [sc / len(loader) for sc in metrics_score]\n",
        "    \n",
        "    return epoch_loss, metrics_score\n",
        "    #epoch_loss = epoch_loss/len(loader)\n",
        "    #return epoch_loss,\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device,mode):\n",
        "    epoch_loss = 0.0\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0,0.0,0.0,0.0,0.0]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            score = calculate_metrics(y, y_pred)\n",
        "            metrics_score = list(map(add, metrics_score, score))\n",
        "        epoch_loss = epoch_loss/len(loader)\n",
        "        metrics_score = [sc / len(loader) for sc in metrics_score]\n",
        "    return epoch_loss, metrics_score\n",
        "\n",
        "def test(model, loader, loss_fn, device,mode):\n",
        "    epoch_loss = 0.0\n",
        "    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0,0.0,0.0,0.0,0.0]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            score = calculate_metrics(y, y_pred)\n",
        "            metrics_score = list(map(add, metrics_score, score))\n",
        "        epoch_loss = epoch_loss/len(loader)\n",
        "        metrics_score = [sc / len(loader) for sc in metrics_score]\n",
        "    return epoch_loss, metrics_score\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    seeding(42)\n",
        "\n",
        "    \"\"\" Directories \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Program/files\")\n",
        "\n",
        "    \"\"\" Load test dataset \"\"\"\n",
        "    data_dir=\"/content/drive/MyDrive/Augmented_data/train/\"\n",
        "    test_x = sorted(glob(\"/content/drive/MyDrive/Augmented_data/test/image/*\"))\n",
        "    test_y = sorted(glob(\"/content/drive/MyDrive/Augmented_data/test/mask/*\"))\n",
        "    \n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    num_epochs=2\n",
        "    batch_size=2\n",
        "    k=10\n",
        "    lr = 1e-3\n",
        "    checkpoint_path= \"/content/drive/MyDrive/Program/files/checkpoint64.pth\"\n",
        "   \n",
        "    \"\"\" Dataset and loader \"\"\"\n",
        "    \n",
        "    train_loader, valid_loader=get_train_valid_loader(data_dir=data_dir,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      random_seed=False,\n",
        "                                                      kfolde=k,\n",
        "                                                      shuffle=True,\n",
        "                                                      show_sample=False,\n",
        "                                                      num_workers=1,\n",
        "                                                      pin_memory=False,\n",
        "                                                      shuffelthevaluditation=1)\n",
        "    print(\"train data loader\",len(train_loader))\n",
        "    print(\"valid data loader\",len(valid_loader)) \n",
        "    \n",
        "    test_dataset=DriveDataset(test_x,test_y)\n",
        "    test_loader = DataLoader(\n",
        "                              dataset=  test_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                              shuffle=False,\n",
        "                              num_workers=1\n",
        "                         )\n",
        "\n",
        "    loaded_checkpoint=torch.load(checkpoint_path)\n",
        "    #epoch=loaded_checkpoint[\"epoch\"]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = build_unet(dropRate=0.09)\n",
        "    model = model.to(device)\n",
        "    print('Number of model parameters: {}'.format(\n",
        "      sum([p.data.nelement() for p in model.parameters()])))  \n",
        "        \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "\n",
        "    model.load_state_dict(loaded_checkpoint[\"model_state\"])#loading the trained model\n",
        "    optimizer.load_state_dict(loaded_checkpoint[\"optim_state\"])\n",
        "\n",
        "    loss_fn = DiceBCELoss().to(device)\n",
        "\n",
        "    #creating dictionary to save checkpoints\n",
        "    checkpoint={\n",
        "              #\"epoch\":10,\n",
        "              \"model_state\":model.state_dict(),\n",
        "              \"optim_state\":optimizer.state_dict()\n",
        "                }\n",
        "\n",
        "    \"\"\"oneing/creating required csv files for recording data\"\"\"\n",
        "       \n",
        "    with open('/content/drive/MyDrive/TrainResult/train_64_512.csv','a+') as td:\n",
        "        training_details=csv.writer(td)\n",
        "        with open('/content/drive/MyDrive/TrainResult/val_64_512.csv','a+') as vd:\n",
        "            val_details=csv.writer(vd)\n",
        "            with open('/content/drive/MyDrive/TestResult/Test_64_512.csv','a+') as tsd:\n",
        "                Test_details=csv.writer(tsd)\n",
        "                \"\"\" Training the model \"\"\"\n",
        "                best_valid_loss = float(\"inf\")                  \n",
        "                best_train_accuracy=0.0\n",
        "                best_val_accuracy=0.0\n",
        "                best_test_accuracy=0.0\n",
        "                best_auc=0.0\n",
        "                best_sp=0.0\n",
        "                best_se=0.0\n",
        "                best_f1=0.0\n",
        "                best_iou=0.0\n",
        "                best_dc=0.0\n",
        "                              \n",
        "\n",
        "                for epoch in range(num_epochs):\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    train_loss, train_score= train(model, train_loader, optimizer, loss_fn, device)\n",
        "                    print(\"Training Done\")\n",
        "                    valid_loss, val_score = evaluate(model, valid_loader, loss_fn, device,'valid')\n",
        "                    print(\"Validation Done\")\n",
        "                    test_loss, test_score = test( model,test_loader, loss_fn, device,'Test')\n",
        "                    print(\"Test Done\")\n",
        "                    \"\"\" Saving the model \"\"\"\n",
        "                    if valid_loss < best_valid_loss:\n",
        "                      data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "                      print(data_str)\n",
        "\n",
        "                      best_valid_loss = valid_loss\n",
        "                      torch.save(checkpoint, checkpoint_path)#saving the checkpoiunts\n",
        "                                  \n",
        "                    if best_train_accuracy< train_score[4]:\n",
        "                      best_train_accuracy = max(train_score[4],best_train_accuracy)                    \n",
        "                      print(f\"Best train Accuracy:{best_train_accuracy:1.4f}\")\n",
        "                    if best_val_accuracy < val_score[4]:\n",
        "                      best_val_accuracy = max(val_score[4],best_val_accuracy)                    \n",
        "                      print(f\"Best Validation Accuracy:{best_val_accuracy:1.4f}\")\n",
        "                    if best_test_accuracy < test_score[4]:\n",
        "                      best_test_accuracy = max(test_score[4],best_test_accuracy)                    \n",
        "                      print(f\"Best Test Accuracy:{best_test_accuracy:1.4f}\")\n",
        "                    if best_auc < test_score[7]:\n",
        "                      best_auc = max(test_score[7],best_auc) \n",
        "                    if best_sp < test_score[2]:\n",
        "                      best_sp = max(test_score[2],best_sp) \n",
        "                    if best_se < test_score[6]:\n",
        "                      best_se = max(test_score[6],best_se) \n",
        "                    if best_f1 < test_score[1]:\n",
        "                      best_f1= max(test_score[1],best_f1) \n",
        "                    if best_iou < test_score[0]:\n",
        "                      best_iou = max(test_score[0],best_iou) \n",
        "                    if best_dc < test_score[8]:\n",
        "                      best_dc = max(test_score[8],best_dc) \n",
        "\n",
        "                     \n",
        "\n",
        "                    end_time = time.time()\n",
        "                    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "                                  \n",
        "                    data_str = f'Epoch: {epoch+1+11:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "                    data_str += f'\\tTrain Loss: {train_loss:.3f}'\n",
        "                    data_str += f'\\tVal. Loss: {valid_loss:.3f}'\n",
        "                    data_str += f'\\tTest. Loss: {test_loss:.3f}'\n",
        "                    print(data_str)\n",
        "\n",
        "                    \n",
        "                    t_jaccard = train_score[0]\n",
        "                    t_f1 = train_score[1]\n",
        "                    t_recall = train_score[2]\n",
        "                    t_precision = train_score[3]\n",
        "                    t_acc = train_score[4]\n",
        "                    t_sp=train_score[5]\n",
        "                    t_se=train_score[6]\n",
        "                                                        \n",
        "                    jaccard = val_score[0]\n",
        "                    f1 = val_score[1]\n",
        "                    recall = val_score[2]\n",
        "                    precision = val_score[3]\n",
        "                    acc = val_score[4]\n",
        "                    sp=val_score[5]\n",
        "                    se=val_score[6]\n",
        "\n",
        "                                  \n",
        "                    training_details.writerow([str(epoch+1+11),float(train_loss),float(t_jaccard),float(t_f1),float(t_recall),float(t_precision),float(t_acc),float(t_sp),float(t_se)])                    \n",
        "                    val_details.writerow([str(epoch+1+11),float(valid_loss),float(jaccard),float(f1),float(recall),float(precision),float(acc),float(sp),float(se)])                                  \n",
        "                    Test_details.writerow([str(epoch+1+11),float(test_loss),float(test_score[0]),float(test_score[1]),float(test_score[2]),float(test_score[3]),float(test_score[4]),float(test_score[5]),float(test_score[6]),float(test_score[7]),float(test_score[8])])                                           \n",
        "                    print(f\"\\n\\ttr_acc: {t_acc:1.4f}\\tval_acc: {acc:1.4f}\\tTest_acc: {test_score[4]:1.4f}\")\n",
        "                    print(f\"\\n\\tValidation_Jaccard: {jaccard:1.4f} \\tF1: {f1:1.4f} \\tRecall: {recall:1.4f} \\tPrecision: {precision:1.4f} \\tSP: {sp:1.4f}\\tSE: {se:1.4f}\")\n",
        "                    print(f\"\\n\\tTest_Jaccard: {test_score[0]:1.4f} \\tF1: {test_score[1]:1.4f} \\tRecall: {test_score[2]:1.4f} \\tPrecision: {test_score[3]:1.4f} \\tSP: {test_score[5]:1.4f}\\tSE: {test_score[6]:1.4f}\\tAUC: {test_score[7]:1.4f}\\tDC: {test_score[8]:1.4f}\")\n",
        "\n",
        "                    train_loader, valid_loader =get_train_valid_loader(kfolde=k,shuffelthevaluditation=(epoch%k)+1,random_seed=False,data_dir=data_dir,batch_size=batch_size, shuffle=True)\n",
        "        print(\"Completed Successfully\")\n",
        "        print(f\"Best train Accuracy:{best_train_accuracy:1.4f}\")\n",
        "        print(f\"Best Validation Accuracy:{best_val_accuracy:1.4f}\")\n",
        "        print(f\"Best Test Accuracy:{best_test_accuracy:1.4f}\")\n",
        "        print(f\"Best Test IOU:{best_iou:1.4f}\")\n",
        "        print(f\"Best Test F1-Score:{best_f1:1.4f}\")\n",
        "        print(f\"Best Test Sensitivity:{best_se:1.4f}\")\n",
        "        print(f\"Best Test Specificity:{best_sp:1.4f}\")\n",
        "        print(f\"Best Test AUC:{best_auc:1.4f}\")\n",
        "        print(f\"Best Test DC:{best_dc:1.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PoRiNfOWnjtz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RIMNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}